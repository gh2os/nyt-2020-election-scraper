#!/usr/bin/env python3

import csv
import collections
import datetime
import email.utils
import git
import hashlib
import itertools
import os
import simdjson
import subprocess
import json
from textwrap import dedent, indent
from tabulate import tabulate
from typing import Dict, Tuple

# Constants and state indexes
AK_INDEX = 0
AZ_INDEX = 3
GA_INDEX = 10
NC_INDEX = 27
NV_INDEX = 33
PA_INDEX = 38

BATTLEGROUND_STATES = ["Alaska", "Arizona", "Georgia", "North Carolina", "Nevada", "Pennsylvania"]
STATE_INDEXES = range(51)  # 50 States + DC

CACHE_DIR = '_cache'
CACHE_VERSION = 2

# Define structured data using named tuples
# Define the structure for InputRecord based on parsed JSON
InputRecord = collections.namedtuple(
    'InputRecord',
    [
        'timestamp',
        'state_name',
        'state_abbrev',
        'electoral_votes',
        'candidates',
        'votes',
        'expected_votes',
        'precincts_total',
        'precincts_reporting',
        'counties',
    ]
)

# Function to process JSON data and create InputRecords
def process_json_data(json_data: dict) -> List[InputRecord]:
    records = []
    for race in json_data["races"]:
        # Parse the timestamp from the race data
        updated_at = datetime.datetime.fromisoformat(race["updated_at"].replace("Z", "+00:00"))
        
        # Process each reporting unit (representing a state)
        for unit in race["reporting_units"]:
            # Extract candidate details
            candidates = [
                {
                    "last_name": candidate.get("nyt_id", ""),
                    "votes": candidate["votes"]["total"]
                }
                for candidate in unit["candidates"]
            ]
            
            # Create InputRecord for each state unit
            record = InputRecord(
                timestamp=updated_at,
                state_name=unit["name"],
                state_abbrev=unit["state_abb"],
                electoral_votes=race.get("electoral_votes", 0),
                candidates=candidates,
                votes=unit["total_votes"],
                expected_votes=unit.get("total_expected_vote", 0),  # Fallback if not present
                precincts_total=unit["precincts_total"],
                precincts_reporting=unit["precincts_reporting"],
                counties={}  # Placeholder as counties data is not in the sample
            )
            records.append(record)
    
    return records

# Sample JSON data for testing
sample_json = {
    "races": [
        {
            "updated_at": "2024-11-06T01:18:17.436Z",
            "electoral_votes": 9,
            "reporting_units": [
                {
                    "name": "Alabama",
                    "state_abb": "AL",
                    "total_votes": 752,
                    "precincts_total": 2028,
                    "precincts_reporting": 1,
                    "candidates": [
                        {
                            "nyt_id": "trump-d",
                            "votes": {"total": 516}
                        },
                        {
                            "nyt_id": "harris-k",
                            "votes": {"total": 232}
                        },
                        {
                            "nyt_id": "kennedy-r",
                            "votes": {"total": 2}
                        }
                    ]
                }
            ]
        }
    ]
}

# Test the function with sample data
processed_records = process_json_data(sample_json)
for record in processed_records:
    print(record)

def git_commits_for(path):
    return subprocess.check_output(['git', 'log', "--format=%H", path]).strip().decode().splitlines()

def git_show(ref, name, repo_client):
    commit_tree = repo_client.commit(ref).tree
    return commit_tree[name].data_stream.read()

def git_commits_for(path):
    return subprocess.check_output(['git', 'log', "--format=%H", path]).strip().decode().splitlines()

def git_show(ref, name, repo_client):
    commit_tree = repo_client.commit(ref).tree
    return commit_tree[name].data_stream.read()

# Updated fetch_all_records function
def fetch_all_records():
    commits = git_commits_for("results.json")
    repo = git.Repo('.', odbt=git.db.GitCmdObjectDB)
    out = []

    parser = simdjson.Parser()
    for ref in commits:
        cache_path = os.path.join(CACHE_DIR, ref[:2], ref[2:] + ".json")
        
        # Check cache
        if os.path.exists(cache_path):
            with open(cache_path) as fh:
                try:
                    record = simdjson.load(fh)
                except ValueError:
                    continue
                if record['version'] == CACHE_VERSION:
                    for row in record['rows']:
                        out.append(InputRecord(*row))
                    continue
        
        # Process the latest commit data from git
        blob = git_show(ref, 'results.json', repo)
        json_data = parser.parse(blob)

        # Parse the JSON data using process_json_data function
        rows = process_json_data(json_data)
        out.extend(rows)
        
        # Cache the parsed data
        try:
            os.makedirs(os.path.dirname(cache_path))
        except FileExistsError:
            pass
        with open(cache_path, 'w') as fh:
            simdjson.dump({"version": CACHE_VERSION, "rows": [row._asdict() for row in rows]}, fh)

    # Sort records by timestamp
    out.sort(key=lambda row: row.timestamp)

    # Group by state for easy access
    grouped = collections.defaultdict(list)
    for row in out:
        grouped[row.state_name].append(row)

    return grouped
def compute_hurdle_sma(summarized_state_data, newest_votes, new_partition_pct, trailing_candidate_name):
    hurdle_moving_average = None
    MIN_AGG_VOTES = 30000
    agg_votes = newest_votes
    agg_c2_votes = round(new_partition_pct * newest_votes)
    step = 0
    while step < len(summarized_state_data) and agg_votes < MIN_AGG_VOTES:
        this_summary = summarized_state_data[step]
        step += 1
        if this_summary.new_votes_relevant > 0:
            trailing_candidate_partition = this_summary.trailing_candidate_partition if this_summary.trailing_candidate_name == trailing_candidate_name else this_summary.leading_candidate_partition
            if this_summary.new_votes_relevant + agg_votes > MIN_AGG_VOTES:
                subset_pct = (MIN_AGG_VOTES - agg_votes) / float(this_summary.new_votes_relevant)
                agg_votes += round(this_summary.new_votes_relevant * subset_pct)
                agg_c2_votes += round(trailing_candidate_partition * this_summary.new_votes_relevant * subset_pct)
            else:
                agg_votes += this_summary.new_votes_relevant
                agg_c2_votes += round(trailing_candidate_partition * this_summary.new_votes_relevant)
    if agg_votes:
        hurdle_moving_average = float(agg_c2_votes) / agg_votes
    return hurdle_moving_average

def string_summary(summary):
    thirty_ago = (datetime.datetime.utcnow() - datetime.timedelta(minutes=30))
    bumped = summary.leading_candidate_partition
    bumped_name = summary.leading_candidate_name
    if bumped < summary.trailing_candidate_partition:
        bumped = summary.trailing_candidate_partition
        bumped_name = summary.trailing_candidate_name
    bumped -= 0.50
    visible_hurdle = f'{summary.trailing_candidate_name} needs {summary.hurdle:.2%}' if summary.hurdle > 0 else 'Unknown'
    return [
        f'{summary.timestamp.strftime("%Y-%m-%d %H:%M")}',
        '***' if summary.timestamp > thirty_ago else '---',
        f'{summary.leading_candidate_name} up {summary.vote_differential:,}',
        f'Left (est.): {summary.votes_remaining:,}' if summary.votes_remaining > 0 else 'Unknown',
        f'Î”: {summary.new_votes_formatted} ({f"{bumped_name} +{bumped:5.01%}" if (summary.leading_candidate_partition or summary.trailing_candidate_partition) else "n/a"})',
        f'{summary.precincts_reporting/summary.precincts_total:.2%} precincts',
        f'{visible_hurdle}',
        f'& trends  {f"{summary.hurdle_mov_avg:.2%}" if summary.hurdle_mov_avg else "n/a"}'
    ]

def generate_txt_output(path, summarized, states_updated):
    with open(path, "w") as f:
        print(tabulate([
            ["Last updated:", scrape_time.strftime("%Y-%m-%d %H:%M UTC")],
            ["Latest batch received:", f"({', '.join(states_updated)})"],
            ["Web version:", "https://example.com"]
        ]), file=f)
        for state, timestamped_results in sorted(summarized.items()):
            print(f'\n{state} - Total Votes:', file=f)
            print(tabulate([string_summary(summary) for summary in timestamped_results]), file=f)

def generate_html_output(path, html_table, states_updated, other_page_html):
    html_template = "<!-- Generated automatically -->\n\n"
    with open("battleground-state-changes.html.tmpl", "r", encoding='utf8') as f:
        html_template += f.read()
    TEMPLATE_HASH = hashlib.sha256(html_template.encode('utf8')).hexdigest()

    with open(path,"w", encoding='utf8') as f:
        page_metadata = json.dumps({
            "template_hash": TEMPLATE_HASH,
            "states_updated": states_updated,
        })
        html = html_template.replace('{% TABLES %}', "\n".join(html_table))
        html = html.replace('{% OTHER_PAGE_TEXT %}', other_page_html)
        f.write(html)

def generate_csv_output(path, summarized):
    with open(path, 'w') as csvfile:
        wr = csv.writer(csvfile)
        wr.writerow(('state',) + IterationSummary._fields)
        for state, results in summarized.items():
            for row in results:
                wr.writerow((state,) + row)

def generate_rss_output(path, summarized):
    with open(path, 'w') as rssfile:
        print(dedent(f'''
            <?xml version="1.0" encoding="UTF-8"?>
            <rss version="2.0">
            <channel>
              <title>Election Results Feed</title>
              <link>https://example.com</link>
              <description>Latest results</description>
              <lastBuildDate>{email.utils.formatdate(batch_time.timestamp())}</lastBuildDate>
        '''), file=rssfile)

        for state, results in summarized.items():
            if not results:
                continue
            timestamp = results[0].timestamp.timestamp()
            print(indent(dedent(f'''
                <item>
                    <description>{state}: {results[0].leading_candidate_name} +{results[0].vote_differential}</description>
                    <pubDate>{email.utils.formatdate(timestamp)}</pubDate>
                    <guid isPermaLink="false">{state}@{timestamp}</guid>
                </item>
            '''), "  "), file=rssfile)

        print(dedent('''
             </channel>
            </rss>'''), file=rssfile)

# Capture the scrape time
scrape_time = datetime.datetime.utcnow()
records = fetch_all_records()

# Summarized data dictionaries
summarized = {}
state_formatted_name = {}
state_abbrev = {}

# Generate outputs
generate_txt_output("battleground-state-changes.txt", summarized, BATTLEGROUND_STATES)
generate_csv_output("battleground-state-changes.csv", summarized)
generate_rss_output("battleground-state-changes.xml", summarized)

print("Script completed successfully.")